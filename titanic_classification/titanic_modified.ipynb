{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc14f86",
   "metadata": {},
   "source": [
    "# Titanic Comptetion\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1. Problem Statement\n",
    "\n",
    "This competition revolves around a classification problem for the titanic dataset. Apparently, not everyone on the titanic ship died during the horrifying event you all know. Although it seems that people should have survived at random, there seems to have been a correlation between certain attributes of the passengers and their chance of survival!\n",
    "Here are some of these attributes:\n",
    "\n",
    "| **Attribute**  |              **Description**           | **Column in dataset**  |\n",
    "|----------------|:---------------------------------------|:-----------------------|\n",
    "|  Ticket class  |  Three possible classes: 1st, 2nd, 3rd |        *pclass*        |\n",
    "|     Gender     |  The sex of the passenger              |         *sex*          |\n",
    "|      Age       |  Age of the passenger in years         |         *age*          |\n",
    "|   Siblings     |  *Number* of siblings aboard Titanic   |         *sibsp*        |\n",
    "|    Parents     |  *Number* of parents aboard Titani     |         *parch*        |\n",
    "|     Ticket     |  Ticket number of the passenger        |         *ticket*       |\n",
    "|      Fare      |  Passenger fare                        |         *fare*         |\n",
    "|      Cabin     |  Cabin number of the passenger         |         *cabin*        |\n",
    "| Port of entry  |  The station at which passneger boarded|         *embarked*     |\n",
    "\n",
    "### 1.2. Dataset\n",
    "\n",
    "There are two ```.csv``` files to this problem which are placed in the ```./data/``` directory:\n",
    "\n",
    "- **train.csv**: which contains the attributes in the above table along with an extra column called *survived* which indicates whether the passenger has survived or not (1 for survived and 0 for not survivied). This dataset is meant to be used for model trainig.\n",
    "- **test.csv**: which contains only the attributes of the above table without the *survived* column since this dataset is meant for testing the model and the answers for it are hidden. The performance of your model on this dataset will be used to rank the model in the competition.\n",
    "\n",
    "### 1.3. Output\n",
    "\n",
    "The output of the model is the answers it gives for the examples provided in test.csv file. Therefore, the output is a ```.csv``` file that contains two columns:\n",
    "\n",
    "- **PassengerID**: which is the passenger ID of the passengers in the test.csv file.\n",
    "- **Survived**: which is 1 for people who survived and 0 otherwise. These values are the predictions given by the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70973d37",
   "metadata": {},
   "source": [
    "# 2. Data Insight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a78157e",
   "metadata": {},
   "source": [
    "## 2.1. Loading data and importing modules\n",
    "\n",
    "First we need to import the necessary modules and packages:\n",
    "\n",
    "- **pandas**: for data manipulation\n",
    "- **np**: for array operations\n",
    "- **matplotlib.pyplot**: for data visualization\n",
    "- **seaborn**: for data visualization\n",
    "- **scipy**: for statistical operations.\n",
    "\n",
    "Then, we set the appropriate directories to read data from and finally we read data and load them onto a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d481ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the directories.\n",
    "CURRENT_DIR = './'\n",
    "DATA_DIR = CURRENT_DIR + 'data/'\n",
    "TRAIN_FILE_NAME = 'titanic_train.csv'\n",
    "TEST_FILE_NAME = 'titanic_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a578ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data and print the first few rows\n",
    "train_ds = pd.read_csv(DATA_DIR + TRAIN_FILE_NAME)\n",
    "print(f'The dataset has {train_ds.shape[0]} data points.')\n",
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f087154",
   "metadata": {},
   "source": [
    "## 2.2 Data Analysis\n",
    "\n",
    "Next, we need to analyze our data to understand them and gain more insight into them. This is an important step since it allows for better fabrication of the prediction model. The steps in analysis include:\n",
    "\n",
    "- Checking for duplicate values and deciding how to manage them.\n",
    "- Checking Null (or NaN) values and deciding how to manage them.\n",
    "- Understanding the distribution of each attribute.\n",
    "- Understanding the relationship between different attributes and the target variable (i.e., Survived).\n",
    "- Understanding the relationship between input attributes to understand if there exists any redundant information in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b1e96",
   "metadata": {},
   "source": [
    "### 2.2.1 Duplicate value handling\n",
    "\n",
    "Handling duplicate values is important since these values introduce a bias to the final system that makes the prediction model to give more attention to these repeated values. This in turn can cause overfitting that prevents the prediction system to generalize well to unseen data points.\n",
    "\n",
    "Based on the tables shown above, there are two ways that we can check if a row is duplicate:\n",
    "\n",
    "- Check by the name of the passengers\n",
    "- Check by the ticket number of passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf66bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicate values among person names\n",
    "duplicate_num = len(train_ds.duplicated(subset=['Name'])[train_ds.duplicated(subset=['Name'])])\n",
    "if duplicate_num > 0:\n",
    "    print(f'We have {duplicate_num} duplicate persons in the dataset')\n",
    "else:\n",
    "    print('There are no duplicate persons based on name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f5b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the duplicate values based on tickets\n",
    "train_ds[train_ds.duplicated(subset=['Ticket'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa050e1",
   "metadata": {},
   "source": [
    "Analyzing the ticket number of passengers shows that there are multiple passengers with the same ticket number. However, the result of the next cell (for a single ticket number) shows that its because all the family members of a certain family have the similar ticket number. Therefore, we can conclude that there are now duplicate passenger information in this table, thus, no need to handle duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11182597",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[train_ds['Ticket'] == '382652']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982dc35",
   "metadata": {},
   "source": [
    "### 2.2.2 Handling missing values\n",
    "\n",
    "Now we need to address the missing values in the dataset. The next cell shows how many missing values is present for each column of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c101b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in train_ds.columns:\n",
    "    print(f'{c} column has {train_ds[c].isna().sum()} NaN values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eebc65",
   "metadata": {},
   "source": [
    "From the above cell we can see that Most columns do not have missing values. From the columns that do have missing values (i.e., Age, Cabin, Embarked), *Cabin* has the most number of missing values while *Embarked* has the least number of missing values. Now we need to decide either to fill the missing values with some other value or to completely remove a certain column. Since *Cabin* column has a missing value for nearly 75% of the rows and since by our own insight, we can see that cabin number may not introduce that much information regarding whether a person was survived or not, we can decide to remove this column for now. Also, we can get rid of ticket number and name since they are also likely to introduce only a negligible amount of information to the final prediction model.\n",
    "\n",
    "As fo *Age* and *Embarked* we can replace them by their median and mode, respectively. We choose median for *Age* since it's a numerical value and mode for *Embarked* since it is a categorical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaNs in Age and Embarked columns with median and most frequent values\n",
    "train_ds[['Age']] = train_ds[['Age']].fillna(value=train_ds['Age'].median())\n",
    "train_ds[['Embarked']] = train_ds[['Embarked']].fillna(train_ds['Embarked'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412382d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losing the useless columns\n",
    "train_ds.drop(['Cabin', 'Name', 'Ticket', 'PassengerId'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601ce92",
   "metadata": {},
   "source": [
    "### 2.2.3 Univariate Analysis\n",
    "\n",
    "Now we need to analyze each column (or attribute) individually. The best way to this is to understand what is the distribution of all possible values of each attribute. We plot the distibution of *numerical* and *categorical* attibutes in separate cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4bd17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL = ['Sex', 'Pclass', 'Survived', 'Embarked']\n",
    "NUMERICAL = ['Age', 'Parch', 'SibSp', 'Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Numerical parameters\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "train_ds.hist(column=NUMERICAL, figsize=(10.0, 10.0), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d0579",
   "metadata": {},
   "source": [
    "As it can be seen from the cell above, the distribution of data over numerical attributes is fairly skewed. Since most ML models work better with normally distributed data or at least data that is zero-centered. So, this is an indication that we need to transfrom our numerical attributes before feeding them into any ML model. Also, one thing that we should note here is that for *SibSp* (or number of siblings on board), *Fare* and *Parch* (or number of parents on board), most people had a value that are very close to 0. Especially for *SibSp* and *Parch* that have discrete values, this means that had a value of 0. When the value of a certain attribute is mostly repeated at a certain value, it means that it introduces little information to the system (less entropy). Although a covariate analysis in the next sections can give a better idea whether each of these attributes introduce significant information towards our target variable (i.e., Survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b27a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(10.0, 10.0))\n",
    "for c in range(len(CATEGORICAL)):\n",
    "    plt.subplot(2, 2, c+1)\n",
    "    train_ds[CATEGORICAL[c]].value_counts().plot(kind='bar')\n",
    "    plt.title(f'{CATEGORICAL[c]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9014c2",
   "metadata": {},
   "source": [
    "Finally for the categorical attribiutes we can see that the data is less unbalanced. Of course more people have died than survived and apparently there were more men on board than women. The cell above also shows that most of the people had acquired a ticket class of 3 and they mostly boarded the ship from the *s* station."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad5d5d",
   "metadata": {},
   "source": [
    "### 2.2.2 Bivariate Analysis\n",
    "\n",
    "Now it is time to perform an analysis to better understand the relationship between different attributes and the target variable (i.e., Survived). First we have to pay attention that our target variable is a categorical one and not a numerical variable (i.e., eitehr survived and not survived). The fact that we show these two possible values with integers 1 and 0 does not make this variable a numerical one. Secondly, we should remember that between other attributes, some are categorical (sucha as Sex) and some are numerical (such as Age).\n",
    "\n",
    "What we want to understand here is (for each attribute) whether there exists a meaningful relationship between the target variable and the attribute. In other words (and statistically speaking), if we assume that there is no relationship between the attribute and the target variable (i.e., null hypothesis), are we able to reject this null hypothesis or not? If yes, then we have successfully shown that there exists a meaningful relationship between the attribute and the target variable. We fail to reject the null hypothesis and it means that we failed to show that the two are related.\n",
    "\n",
    "For categorical variables (and given that the target variable is also categorical), we can use a chisquare test on the ```crosstab``` table of the attribute and the target variable. Chi-Square gives a P-value that is the **probability that the null hypothesis holds**. In other words, if P-value is 0.05 percent it means that by a confidence interval of 95% we can reject the null hypothesis. This seems like a reasonable number and we use the same number as a threshold to reject the null hypothesis in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1f25e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in CATEGORICAL:\n",
    "    p_value = chi2_contingency(pd.crosstab(index=train_ds[c], columns=train_ds['Survived']))[1]\n",
    "    if p_value < 0.05:\n",
    "        print(f'Null hypothesis rejected -> Survived and {c} columns are related')\n",
    "    else:\n",
    "        print(f'We failed to reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84d242",
   "metadata": {},
   "source": [
    "As we can see from the cell above, all of our categorical attributes seem to have a meaningful relationship with the target variable so it is wise to keep them in the dataset.\n",
    "\n",
    "Now we have to also understand the relationship that exists between our numerical attributes and the target variable. To do this, I prefer to use a visualization method. We will analyze each of the attributes one at a time. First, we take a look at the relationship between *Age* and *Survived* in the next cell. For this purpose, we plot a stacked bar chart and a scatterplot with each dot painted as either blue (demised) or orange (survived)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e4ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A stacked bar to understand the correlation between age attribute and survived attribute\n",
    "figure = plt.figure(figsize=(20.0, 10.0))\n",
    "\n",
    "# Stacked bar representation\n",
    "min_age = train_ds['Age'].min()\n",
    "max_age = train_ds['Age'].max()\n",
    "age_bins = np.linspace(min_age, max_age, 10)\n",
    "survived = train_ds[train_ds['Survived']==1].groupby(pd.cut(train_ds['Age'], age_bins))['Age'].count()\n",
    "n_survived = train_ds[train_ds['Survived']==0].groupby(pd.cut(train_ds['Age'], age_bins))['Age'].count()\n",
    "plt.subplot(1,2,1)\n",
    "plt.bar(age_bins[1:], survived, color='blue', width=0.5, label='survived')\n",
    "plt.bar(age_bins[1:], n_survived, color='red', bottom=survived, width=0.5, label='not survived')\n",
    "plt.title('Age-Survived distribution in bar format')\n",
    "plt.xlabel('Age ranges')\n",
    "plt.ylabel('Counts')\n",
    "plt.legend()\n",
    "\n",
    "# Scatterplot representation\n",
    "plt.subplot(1,2,2)\n",
    "for i in range(1, 7):\n",
    "    plt.axhline(y=i*10, color='r', linestyle='-')\n",
    "sns.scatterplot(data=train_ds, x=train_ds.index, y='Age', hue='Survived', legend='auto')\n",
    "plt.title('Age-Survived distribution in scatter format (orange survived, blue demised)')\n",
    "plt.ylabel('Age')\n",
    "plt.xlabel('Index')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e37590",
   "metadata": {},
   "source": [
    "As we can see from the cell above, there seems to be a relationship between the age and the chance of survival. It seems that younger people had a greater change of survival as compared to older people.\n",
    "\n",
    "Now, we plot the same stacked bar plot for number of parents and number of siblings on board in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between number of siblings and survived attribute.\n",
    "figure = plt.figure(figsize=(20.0, 10.0))\n",
    "counter = 1\n",
    "for c in ['SibSp', 'Parch']:\n",
    "    sib_bins = train_ds[c].unique()\n",
    "    survived_counts = train_ds[train_ds['Survived']==1].groupby(c)[c].count()\n",
    "    n_survived_counts = train_ds[train_ds['Survived']==0].groupby(c)[c].count()\n",
    "    survived = []\n",
    "    n_survived = []\n",
    "    for i in sib_bins:\n",
    "        if survived_counts.get(i):\n",
    "            survived.append(survived_counts.get(i))\n",
    "        else:\n",
    "            survived.append(0)\n",
    "\n",
    "        if n_survived_counts.get(i):\n",
    "            n_survived.append(n_survived_counts.get(i))\n",
    "        else:\n",
    "            n_survived.append(0)\n",
    "    \n",
    "    plt.subplot(1, 2, counter)\n",
    "    counter += 1\n",
    "    \n",
    "    plt.bar(sib_bins, survived, color='blue', width=0.5, label='survived')\n",
    "    plt.bar(sib_bins, n_survived, color='red', bottom=survived, width=0.5, label='not survived')\n",
    "    if c == 'SibSp':\n",
    "        plt.title('Stacked bar relationship between # siblings and survived attribute.')\n",
    "        plt.xlabel('# Siblings')\n",
    "    else:\n",
    "        plt.title('Stacked bar relationship between # parents and survived attribute.')\n",
    "        plt.xlabel('# parents')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cb2ed",
   "metadata": {},
   "source": [
    "Again from the cell above, there seems to be a meaningful relationship between the number of parents/siblings and the chance of survival. Specifically, it seems that thos who had 1, 2, 3 parents had a higher chance of survival as compared to those who had no parent. Also, the number of siblings seems to have had an effect on chance of survival and those who had one or two siblings on board seem to have had a higher chance of survival than any other passenger (maybe they helped each other better whereas those with higher number of siblings had to struggle so much to help each other that eventually they all ended up drowning).\n",
    "\n",
    "Lastly, we can take a look at the effect of *Fare* on the chance of survival. We can do so by plotting a scatterplot in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a scatter plot to show the dependency of fare and survived attribute\n",
    "plt.figure(figsize=(10.0, 10.0))\n",
    "plt.axhline(y=200, color='r', linestyle='-')\n",
    "plt.axhline(y=100, color='r', linestyle='-')\n",
    "sns.scatterplot(data=train_ds, x=train_ds.index, y='Fare', hue='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202203fc",
   "metadata": {},
   "source": [
    "Once again we can see that poeple who had paid higher fairs seem to have had a higher chance of survival as compared to those who paid lower fairs (maybe there was a protocol for the crew of the ship to first aid the richer people in case of emergency).\n",
    "\n",
    "All in all we can conclude that all of our attributes have a meaningful relationship with the target variable and we should keep them in our dataset for modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3a364",
   "metadata": {},
   "source": [
    "## 2.3 Data Manipulation\n",
    "\n",
    "No it is time to prepare our dataset so that it can be fed to a Machine Learning model. Most of the ML models that we know of are actually statistical models that look into the underlying statistical pattern and try to figure it out. Therefore, any data or attribute that is fed to them should be in number format so that the algorithm can understand it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83193a7",
   "metadata": {},
   "source": [
    "### 2.3.1 One-hot Encoding\n",
    "As you have already seen, some of the attributes in our dataset are actually categorical values that need to be converted into numerical values (i.e., assigning a numerical value to each category.\n",
    "\n",
    "For example, the *Embarked* attribute has three possible values which are *C*, *Q* and *S*. Although we can simply assign numbers 1, 2 and 3 to these categories, it is better to represent this attribute with a one-hot encoded vector. This is because *C*, *Q* and *S* probably have no connection with each other in real life while numbers 1, 2 and 3 have a logical and arithmetic relation to each other. Therefore, to better capture this independence, it is better to use one-hot encoding. As an example, if the *Embarked* attribute for a certain data point is *S*, then we can represent the *Embarked* attribute for that data point with the [0, 0, 1] vector where the first element represents *C* value, the second element represents *Q* value and the third element represents *S* value. We need to do this for *Sex* and *Pclass* attributes as well since they are also categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd15e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for the categorical attributes\n",
    "one_hot_cols = ['Sex', 'Embarked', 'Pclass']\n",
    "concat_dfs = [train_ds]\n",
    "\n",
    "# Creating a one-hot encoded version of categorical columns\n",
    "# which will result in a new DataFrame which will be appended\n",
    "# to a collection of dataframes that should later be concatenated\n",
    "# with each other\n",
    "for c in one_hot_cols:\n",
    "    concat_dfs.append(pd.get_dummies(train_ds[c]))\n",
    "\n",
    "# Creating an encoded training dataset\n",
    "train_ds_enc = pd.concat(concat_dfs, axis=1)\n",
    "\n",
    "# Dropping the non-encoded columns\n",
    "for c in one_hot_cols:\n",
    "    train_ds_enc.drop(c, axis=1, inplace=True)\n",
    "\n",
    "# Showing the one-hot encoded dataframe\n",
    "train_ds_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f7e5a3",
   "metadata": {},
   "source": [
    "### 2.3.2 Train-Dev Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and dev set since the test dataset is already given\n",
    "x_features = list(train_ds_enc.columns)\n",
    "x_features.remove('Survived')\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(train_ds_enc[x_features], train_ds_enc['Survived'], test_size=0.3,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e094e2",
   "metadata": {},
   "source": [
    "### 2.3.3 Data Normalization and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac691e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the numerical values\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[NUMERICAL])\n",
    "X_train_norm = scaler.transform(X_train[NUMERICAL])\n",
    "X_dev_norm = scaler.transform(X_dev[NUMERICAL])\n",
    "X_train[NUMERICAL] = X_train_norm\n",
    "X_dev[NUMERICAL] = X_dev_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bfc72",
   "metadata": {},
   "source": [
    "# 3 Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de372038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a logistic regression model\n",
    "log_reg_model = LogisticRegression(penalty='l2', fit_intercept=True, class_weight='balanced', warm_start=False,\n",
    "                                   max_iter=500, solver='saga')\n",
    "\n",
    "log_reg_model.fit(X_train.to_numpy(), y_train)\n",
    "train_score = log_reg_model.score(X_train.to_numpy(), y_train)\n",
    "dev_score = log_reg_model.score(X_dev.to_numpy(), y_dev)\n",
    "\n",
    "print(f\"The training accuracy is {train_score * 100}%\")\n",
    "print(f\"The dev accuracy is {dev_score * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_cv_model = LogisticRegressionCV(penalty='l2', cv=5, fit_intercept=True, class_weight='balanced')\n",
    "log_reg_cv_model.fit(pd.concat([X_train, X_dev]).to_numpy(), pd.concat([y_train, y_dev]))\n",
    "\n",
    "log_reg_cv_model.score(X_train.to_numpy(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f83719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a neural model\n",
    "hidden_layers = [1024, 256, 64, 16, 4]\n",
    "\n",
    "neural_model = MLPClassifier(solver='adam', beta_1=0.9, alpha=1e-2, hidden_layer_sizes=hidden_layers,\n",
    "                             max_iter=10**3, learning_rate_init=1e-4, random_state=42)\n",
    "\n",
    "neural_model.fit(X_train.to_numpy(), y_train)\n",
    "\n",
    "train_score = neural_model.score(X_train.to_numpy(), y_train)\n",
    "dev_score = neural_model.score(X_dev.to_numpy(), y_dev)\n",
    "\n",
    "print(f\"The training accuracy is {train_score * 100}%\")\n",
    "print(f\"The dev accuracy is {dev_score * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_values = list(range(2, 10))\n",
    "\n",
    "train_score_per_depth = []\n",
    "dev_score_per_depth = []\n",
    "\n",
    "for d in max_depth_values:\n",
    "    # Building a random forest model\n",
    "    rf_model = RandomForestClassifier(max_depth=d, random_state=0)\n",
    "    rf_model.fit(X_train.to_numpy(), y_train)\n",
    "\n",
    "    train_score = rf_model.score(X_train.to_numpy(), y_train)\n",
    "    dev_score = rf_model.score(X_dev.to_numpy(), y_dev)\n",
    "\n",
    "    train_score_per_depth.append(train_score * 100)\n",
    "    dev_score_per_depth.append(dev_score * 100)\n",
    "    \n",
    "plt.plot(max_depth_values, train_score_per_depth, label='Train Set Scores', color='blue')\n",
    "plt.plot(max_depth_values, dev_score_per_depth, label='Dev Set Scores', color='red')\n",
    "plt.title('Accuracy as a function of maximum depth of the tree')\n",
    "plt.xlabel('Maximum Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f80528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
